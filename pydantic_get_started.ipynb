{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rikki424/pydantic-from-scratch/blob/main/pydantic_get_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Get Started with PyDantic AI\n",
        "\n",
        "Ever used FastAPI, LangChain, or the OpenAI Python SDK? Then you've already used [Pydantic](https://pydantic.dev/) under the hood. Pydantic is Python's go-to library for data validation, used by thousands of packages to ensure data matches expected types and formats.\n",
        "What makes Pydantic special is its use of standard Python type hints. Instead of learning a new syntax, you just write regular Python code with type annotations, and Pydantic handles the validation:\n",
        "\n",
        "```python\n",
        "from pydantic import BaseModel\n",
        "\n",
        "class User(BaseModel):\n",
        "    name: str\n",
        "    age: int\n",
        "    email: str\n",
        "\n",
        "json_string = '{\"name\": \"John Doe\", \"age\": 30, \"email\": \"john@example.com\"}'\n",
        "user = User.parse_raw(json_string)\n",
        "\n",
        "```\n",
        "\n",
        "[Pydantic AI](https://ai.pydantic.dev/) builds on this foundation to make building AI applications just as straightforward. Created by the Pydantic team, it provides a framework for working with large language models (LLMs) that feels natural to Python developers.\n",
        "\n",
        "Key features of Pydantic AI:\n",
        "- Works with major LLM providers (OpenAI, Anthropic, Gemini, Groq)\n",
        "- Uses standard Python for control flow and composition\n",
        "- Validates AI responses using Pydantic models\n",
        "- Supports streaming responses with validation\n",
        "- Includes built-in debugging and monitoring\n",
        "\n",
        "In this tutorial, we'll explore how to use Pydantic AI to build reliable AI applications using familiar Python patterns. Let's get started!"
      ],
      "metadata": {
        "id": "DbzujyM2Pc9O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation\n",
        "\n",
        "The only Python package you need for now is `pydantic_ai`."
      ],
      "metadata": {
        "id": "fjI3ajM8fc5q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "xxQbhxKTypJg"
      },
      "outputs": [],
      "source": [
        "!pip install pydantic_ai -qU"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Colab Environment Ready\n",
        "\n",
        "To make the demo application run, we will also need `nest-asyncio`.\n",
        "\n",
        "Next step is to set up environmental variable `OPENAI_API_KEY` so that the Pydantic AI Agents can pick it up in using OpenAI models."
      ],
      "metadata": {
        "id": "spdjK3wrfmFR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nest-asyncio -qU"
      ],
      "metadata": {
        "id": "4eNx6IKlzZgB"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "BZtoNwxczdux"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai import Agent\n",
        "from pydantic_ai.models.gemini import GeminiModel\n",
        "\n",
        "model = GeminiModel('gemini-1.5-flash', api_key='AIzaSyAeT6Wur-1IBfKMrcxJOR98gXey36jdjM4')\n",
        "agent = Agent(model)"
      ],
      "metadata": {
        "id": "lQUPHTYgzGiM"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pydantic AI Agents\n",
        "\n",
        "Let's start looking into some cool examples of Pydantic AI agents."
      ],
      "metadata": {
        "id": "XRmerIQOgBE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Simplest One\n",
        "\n",
        "Chat with OpenAI `gpt-4o` straight away."
      ],
      "metadata": {
        "id": "z-gg8vLMgKwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai import Agent"
      ],
      "metadata": {
        "id": "Ywyz_i_9y4SC"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run_sync(\"Hey, dude!\")\n",
        "print(response.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muzkRuKj867U",
        "outputId": "1f1ee770-4bb5-4816-e5eb-a471eee0f6c8"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hey dude! What's up?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent with Static Prompt"
      ],
      "metadata": {
        "id": "5xfsoAoAgdOU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent = Agent(model, system_prompt=\"You can only speak Koream\")\n",
        "response = agent.run_sync(\"Hey, dude!\")\n",
        "print(response.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNuOO7a69Z_U",
        "outputId": "8407d78f-28fc-4ad3-dee2-09bebc715ecb"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "이봐, 친구!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent with Dynamic Prompt"
      ],
      "metadata": {
        "id": "IfMRzGDngpPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic_ai import Agent, RunContext"
      ],
      "metadata": {
        "id": "DXr9G0SC96Em"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dynamic_prompt_agent = Agent(model)\n",
        "\n",
        "@dynamic_prompt_agent.system_prompt\n",
        "def set_agent_name(ctx: RunContext[str]) -> str:\n",
        "    return f\" You are a {ctx.deps}.\"\n",
        "\n",
        "response = dynamic_prompt_agent.run_sync(\"Hey, dude! Where are you from?\", deps=\"Canadian\")\n",
        "print(response.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2XHmZke9spd",
        "outputId": "97e6afb4-86c5-48a2-b89e-3cce15bdaf29"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Eh, I'm from Canada!  Where you at?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent with Dependency Type"
      ],
      "metadata": {
        "id": "Q2XEz8rlhAs0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class Player:\n",
        "    name: str\n",
        "    goals: int\n",
        "\n",
        "\n",
        "agent = Agent(\n",
        "    model,\n",
        "    deps_type=Player,\n",
        "    result_type=bool,\n",
        ")\n",
        "\n",
        "@agent.system_prompt\n",
        "def add_player_name(ctx: RunContext[Player]) -> str:\n",
        "    player_name = ctx.deps.name\n",
        "    return f\"The player's name is {player_name}.\"\n",
        "\n",
        "@agent.system_prompt\n",
        "def add_player_goals(ctx: RunContext[Player]) -> str:\n",
        "    goals = ctx.deps.goals\n",
        "    return f\"The player's goals so far is {goals}.\"\n",
        "\n",
        "response = agent.run_sync(\"Hey, dude! Does the player ever score a goal?\", deps=Player(name=\"Messi\", goals=2))\n",
        "print(response.data)\n",
        "\n",
        "response = agent.run_sync(\"Hey, dude! Does the player ever score a goal?\", deps=Player(name=\"Ronaldo\", goals=0))\n",
        "print(response.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8M9B-U5IKGV6",
        "outputId": "134de7a9-0bb6-41b7-c7b5-5af96f6326f2"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Agent with Function Tools\n",
        "\n",
        "Function tools provide a mechanism for models to retrieve extra information to help them generate a response.\n",
        "\n",
        "Developers use decorators `@agent.tool_plain` or `@agent.tool` to define tools."
      ],
      "metadata": {
        "id": "ITkIu7KKhLdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = GeminiModel('gemini-1.5-flash', api_key='AIzaSyAeT6Wur-1IBfKMrcxJOR98gXey36jdjM4')\n",
        "agent = Agent(model)\n",
        "\n",
        "@agent.tool\n",
        "def get_player_goals(ctx: RunContext[str], player_name: str) -> str:\n",
        "    print(f\"Getting the goals of player {player_name} so far\")\n",
        "    if player_name == 'Messi':\n",
        "        return '2'\n",
        "    elif player_name == 'Ronaldo':\n",
        "        return '100'\n",
        "    else:\n",
        "        return '0'\n",
        "\n",
        "response = agent.run_sync(\"Let me know if Messi scored so far\")\n",
        "print(response.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MvdJo3TNZWm",
        "outputId": "6222a632-7024-471c-a1d7-d2219e415784"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I need to know which game or season you're asking about to determine if Messi scored.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.all_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvRxFW8r7LsG",
        "outputId": "1f10bcc9-81b6-44a7-ed74-652c9ad75916"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ModelRequest(parts=[UserPromptPart(content='Let me know if Messi scored so far', timestamp=datetime.datetime(2025, 2, 28, 14, 21, 11, 117018, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'),\n",
              " ModelResponse(parts=[TextPart(content=\"I need to know the player's name to check if they scored.  Please provide the player's name.\\n\", part_kind='text')], model_name='gemini-1.5-flash', timestamp=datetime.datetime(2025, 2, 28, 14, 21, 11, 478667, tzinfo=datetime.timezone.utc), kind='response')]"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent.run_sync(\"Let me know if Saka scored so far\")\n",
        "print(response.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fGvNFz6PPJv",
        "outputId": "f6564220-3337-4b46-b4f4-c77271613f12"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I need to know the player's name to check if they scored.  Can you provide the player's name?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.all_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjURG5FIOqsY",
        "outputId": "000c0b4c-e6eb-4977-ed92-057d3edf74eb"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ModelRequest(parts=[UserPromptPart(content='Let me know if Saka scored so far', timestamp=datetime.datetime(2025, 2, 28, 14, 21, 20, 181055, tzinfo=datetime.timezone.utc), part_kind='user-prompt')], kind='request'),\n",
              " ModelResponse(parts=[TextPart(content=\"I need to know the player's name to check if they scored.  Can you provide the player's name?\\n\", part_kind='text')], model_name='gemini-1.5-flash', timestamp=datetime.datetime(2025, 2, 28, 14, 21, 21, 110906, tzinfo=datetime.timezone.utc), kind='response')]"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ]
    }
  ]
}